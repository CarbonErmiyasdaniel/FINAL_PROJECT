{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarbonErmiyasdaniel/FINAL_PROJECT/blob/main/plant-disease-detection-vgg16\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow gradio -q\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#summary, this code snippet sets up a deep learning environment for image\n",
        "#classification using TensorFlow and Keras, loads necessary libraries for data handling\n",
        "#and visualization, and prepares to create a user interface for model interaction with Gradio.\n",
        "#Each library plays a crucial role in the workflow, from data preprocessing to model evaluation and presentation.\n"
      ],
      "metadata": {
        "id": "elVN4xAyxJek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the beans dataset\n",
        "(train_ds, val_ds, test_ds), info = tfds.load(\n",
        "    \"beans\",\n",
        "    split=['train', 'validation', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "print(\"Classes:\", info.features['label'].names)\n",
        "print(\"Training samples:\", info.splits['train'].num_examples)\n",
        "#In summary, this code snippet effectively loads the beans dataset using TensorFlow Datasets,\n",
        "#separating it into training, validation, and test sets while providing additional metadata about the dataset.\n",
        "#The printed information helps you understand the classes available\n",
        "#and the number of training samples, which are critical for building\n",
        " #and evaluating machine learning models.\n"
      ],
      "metadata": {
        "id": "IO4e4XQkxTIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # normalize\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(format_image).batch(BATCH_SIZE).shuffle(1000)\n",
        "val_ds = val_ds.map(format_image).batch(BATCH_SIZE)\n",
        "test_ds = test_ds.map(format_image).batch(BATCH_SIZE)\n",
        "#In summary, this code snippet prepares the image datasets for training a machine learning model by defining constants for\n",
        "#image size and batch size, creating a function to preprocess images (resize and normalize),\n",
        " # and applying this function to the training, validation, and test datasets. The training dataset is shuffled to prevent overfitting,\n",
        " #  while the validation and test datasets are batched for evaluation purposes.\n",
        "#This preprocessing step is crucial for ensuring that the model receives consistent and properly formatted input data\n"
      ],
      "metadata": {
        "id": "QaWXVVXUx0jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the convolutional base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(3, activation='softmax')  # 3 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "#In summary, this code snippet constructs a convolutional neural network using the VGG16 model as a base for feature extraction.\n",
        "#It freezes the convolutional layers to retain learned features, adds custom layers for classification,\n",
        "#compiles the model with appropriate settings for training, and provides a summary of the model architecture.\n",
        " #This approach leverages transfer learning, allowing you to effectively classify images in a new dataset with reduced training time\n",
        "  #and improved performance.\n"
      ],
      "metadata": {
        "id": "LePd5HAvx5un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")\n",
        "#In summary, this code snippet trains the model using the provided training dataset for 10 epochs\n",
        " #while evaluating its performance on a validation dataset. The training process involves\n",
        " #adjusting the model's weights based on the training data and monitoring its ability to generalize to unseen data through validation.\n",
        " # The history object captures the training metrics, which can be useful for analyzing the model's performance and making adjustments if necessary."
      ],
      "metadata": {
        "id": "lUTOUUPuyD9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Train Acc')\n",
        "plt.plot(val_acc, label='Val Acc')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Train Loss')\n",
        "plt.plot(val_loss, label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.show()\n",
        "#In summary, this code snippet extracts training and validation accuracy and loss metrics from the model's training history and visualizes them using Matplotlib.\n",
        " The resulting plots provide insights into the model's performance over the epochs:\n",
        "\n",
        "#Accuracy Plot: Helps assess how well the model is learning. Ideally, both training and validation accuracy should increase over time.\n",
        " #If the training accuracy continues to rise while validation accuracy plateaus or decreases, it may indicate overfitting.\n",
        "#Loss Plot: Shows how the model's loss decreases over time.\n",
        " # while the validation loss starts to increase, it suggests overfitting.\n",
        "#These visualizations are essential for diagnosing the training process\n",
        "#and making informed decisions about further training, adjustments, or early stopping.\n"
      ],
      "metadata": {
        "id": "kGx9jGFNyoXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=info.features['label'].names))\n",
        "#In summary, this code snippet evaluates the performance of a trained model on a test dataset.\n",
        "# The classification report provides valuable insights into how well the model is performing, including metrics like precision, recall,\n",
        "  # and F1 score, which are essential for assessing the effectiveness of classification models.\n",
        "    #This evaluation step is vital for understanding the model's strengths and weaknesses and for making any necessary adjustments or improvements.\n"
      ],
      "metadata": {
        "id": "xsbDMRzdyy0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"plant_disease_vgg16.h5\")\n",
        "print(\" Model saved as plant_disease_vgg16.h5\")\n",
        "#In summary, this code snippet saves a trained Keras model to a file named plant_disease_vgg16.h5.\n",
        "#Saving the model is crucial for future use, as it allows you to load the model later for predictions\n",
        "# or further training without needing to reconstruct or retrain it.\n",
        "# The confirmation message provides assurance that the model has been saved successfully.\n",
        " #This step is essential in any machine learning workflow, especially when working with complex models that require significant time and resources to train.\n"
      ],
      "metadata": {
        "id": "fjHJxEQizCt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# Reload model if needed\n",
        "model = tf.keras.models.load_model(\"plant_disease_vgg16.h5\")  # Or skip if already in memory\n",
        "\n",
        "# Define constants\n",
        "IMG_SIZE = (224, 224)\n",
        "class_names = ['angular_leaf_spot', 'bean_rust', 'healthy']\n",
        "\n",
        "# Descriptions for better output\n",
        "descriptions = {\n",
        "    \"angular_leaf_spot\": \"Angular Leaf Spot: This is a fungal disease. The leaves have brown spots with yellow edges. It often happens in humid conditions. Recommended action: Improve air circulation and consider fungicide treatment.\",\n",
        "    \"bean_rust\": \"Bean Rust: This is a fungal infection that shows orange or rust-colored spots on the leaves. It reduces the plant's ability to make food. Recommended action: Remove infected leaves and apply fungicides.\",\n",
        "    \"healthy\": \"Healthy Leaf: The leaf looks good with no signs of disease. Keep up the good care!\"\n",
        "}\n",
        "\n",
        "def predict_bean(image):\n",
        "    try:\n",
        "        # Ensure image is valid\n",
        "        if image is None:\n",
        "            return \"No image uploaded.\", \"\", \"\"\n",
        "\n",
        "        img = tf.image.resize(image, IMG_SIZE)\n",
        "        img = tf.expand_dims(img, 0) / 255.0\n",
        "        pred = model.predict(img)\n",
        "        result = class_names[np.argmax(pred)]\n",
        "        confidence = float(np.max(pred))\n",
        "        description = descriptions[result]\n",
        "        return result, f\"{confidence*100:.2f}%\", description\n",
        "    except Exception as e:\n",
        "        return \"Error\", \"Error\", str(e)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_bean,\n",
        "    inputs=gr.Image(type=\"numpy\", label=\"Upload a bean leaf image\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Prediction\"),\n",
        "        gr.Textbox(label=\"Confidence\"),\n",
        "        gr.Textbox(label=\"Description\")\n",
        "    ],\n",
        "    title=\" Bean Disease Detection (VGG16 Transfer Learning)\",\n",
        "    description=\"Upload a bean leaf image to detect if it's healthy or diseased. The system identifies Angular Leaf Spot, Bean Rust, or Healthy leaves.\"\n",
        ")\n",
        "\n",
        "demo.launch()\n",
        "#In summary, this code snippet sets up a web-based interface for a bean disease detection model using Gradio.\n",
        " #Users can upload images of bean leaves,\n",
        " #and the model will predict whether the leaves are healthy or affected by diseases such as Angular Leaf Spot or Bean Rust.\n",
        "  #The interface provides predictions, confidence scores, and detailed descriptions to help users understand the results.\n",
        " # This is a practical application of machine learning in agriculture, facilitating quick diagnosis of plant health issues.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cPzEzCNmzP7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Project Summary\")\n",
        "print(\"----------------------------\")\n",
        "print(\"Model: VGG16 Transfer Learning\")\n",
        "print(\"Dataset: TensorFlow Beans (3 classes)\")\n",
        "print(\"Performance Metric: Accuracy, F1-score\")\n",
        "print(\"Demo: Gradio Web Interface\")\n",
        "print(\"Training completed successfully!\")\n",
        "#This simple yet effective summary provides a clear overview of the project's main components,\n",
        "#including the model used, the dataset, evaluation metrics, the demo interface, and the successful completion of the training process.\n",
        "#It is a useful way to communicate the project's achievements to stakeholders or team members.\n"
      ],
      "metadata": {
        "id": "Nn3DX-QYzjLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}